{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotations from documents have been saved to inter_annotator_agreement_annotaties_thirza.json\n"
     ]
    }
   ],
   "source": [
    "import cassis\n",
    "import json\n",
    "import os\n",
    "\n",
    "base_folder = \"/home/arjan_v_d/LLMarjan/annotations_thirza\"\n",
    "output_file = \"inter_annotator_agreement_annotaties_thirza.json\"\n",
    "\n",
    "# List to hold all sentences from all documents\n",
    "all_documents_sentences = []\n",
    "\n",
    "# Iterate over each subdirectory in the base folder\n",
    "for document_name in os.listdir(base_folder):\n",
    "    document_folder = os.path.join(base_folder, document_name)\n",
    "\n",
    "    # Assuming each folder contains exactly one XML and one XMI file\n",
    "    xml_file = next(os.path.join(document_folder, f) for f in os.listdir(document_folder) if f.endswith('.xml'))\n",
    "    xmi_file = next(os.path.join(document_folder, f) for f in os.listdir(document_folder) if f.endswith('.xmi'))\n",
    "\n",
    "    # Load the TypeSystem\n",
    "    with open(xml_file, 'rb') as f:\n",
    "        typesystem = cassis.load_typesystem(f)\n",
    "\n",
    "    # Load the XMI file\n",
    "    with open(xmi_file, 'rb') as f:\n",
    "        cas = cassis.load_cas_from_xmi(f, typesystem=typesystem)\n",
    "\n",
    "    # Define types\n",
    "    SentenceType = typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence')\n",
    "    NamedEntityType = typesystem.get_type('custom.Span')\n",
    "\n",
    "    # Prepare the output list for this document\n",
    "    sentences_list = []\n",
    "\n",
    "    # Extract sentences and their annotations\n",
    "    for sentence in cas.select(SentenceType):\n",
    "        sentence_text = cas.sofa_string[sentence.begin:sentence.end]\n",
    "        labels = []\n",
    "        \n",
    "        for named_entity in cas.select_covered(NamedEntityType, sentence):\n",
    "            label_text = cas.sofa_string[named_entity.begin:named_entity.end]\n",
    "            labels.append({\n",
    "                \"text\": label_text,\n",
    "                \"start\": named_entity.begin - sentence.begin, \n",
    "                \"end\": named_entity.end - sentence.begin,\n",
    "                \"label\": getattr(named_entity, 'label', 'Unknown')\n",
    "            })\n",
    "        \n",
    "        sentences_list.append({\n",
    "            \"text\": sentence_text,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "    # Append results from this document to the all documents list\n",
    "    all_documents_sentences.extend(sentences_list)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_documents_sentences, f, indent=4)\n",
    "\n",
    "# Optionally print the results\n",
    "print(f\"All annotations from documents have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON data has been saved to filtered_documents_annotaties_no_labels.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the existing JSON data\n",
    "input_file = 'all_documents_annotaties.json'\n",
    "output_file = 'filtered_documents_annotaties_no_labels.json'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter labels for each document to only include those with 'An-Org-Lit'\n",
    "filtered_data = []\n",
    "for document in data:\n",
    "    # Filter labels to keep only 'An-Org-Lit'\n",
    "    filtered_labels = [label for label in document['labels'] if label['label'] == 'An-Org-Lit']\n",
    "    # Append the document with only the filtered labels\n",
    "    filtered_data.append({\n",
    "        \"text\": document['text'],\n",
    "        \"labels\": filtered_labels\n",
    "    })\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "print(f\"Filtered JSON data has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Basic tokenizer that splits on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def create_tags(tokens, span_label):\n",
    "    \"\"\"\n",
    "    Covert span labels to sequence labels.\n",
    "    Language: en/zh\n",
    "    \"\"\"\n",
    "    if span_label != []:\n",
    "        for e in span_label:\n",
    "            e[\"span\"] = e[\"text\"]\n",
    "            e[\"type\"] = e[\"label\"]\n",
    "    span_label = sorted(span_label, key=lambda x: len(x['span']), reverse=True)\n",
    "    span_to_type = {entity['span']: entity['type'] for entity in span_label}\n",
    "    # get words list\n",
    "\n",
    "    # build a tokenizer first\n",
    "    dictionary = dict()\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary[token] = f'[{len(dictionary)}]'\n",
    "    id_string = ' '.join([dictionary[token] for token in tokens])\n",
    "    for entity in span_label:\n",
    "        span_tokens = entity['span'].strip().split(' ')\n",
    "        # validate span token\n",
    "        valid_flag = True\n",
    "        for token in span_tokens:\n",
    "            if token not in dictionary:\n",
    "                valid_flag = False\n",
    "                break\n",
    "        if not valid_flag:\n",
    "            continue\n",
    "        # translate span token into ids\n",
    "        id_substring = ' '.join([dictionary[token] for token in span_tokens])\n",
    "        id_string = ('[sep]' + id_substring + '[sep]').join(id_string.split(id_substring))\n",
    "        # print(id_string)\n",
    "    # convert back to nl\n",
    "    sent = id_string\n",
    "    for token in dictionary:\n",
    "        sent = sent.replace(dictionary[token], token)\n",
    "    words = sent.split('[sep]')\n",
    "\n",
    "    seq_label = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        entity_flag = (word in span_to_type)\n",
    "        word_length = len(word.split(' '))\n",
    "        if entity_flag:\n",
    "            if word_length == 1:\n",
    "                label = [f'{span_to_type[word]}']\n",
    "            else:\n",
    "                label = ([f'{span_to_type[word]}'] * (word_length))\n",
    "        else:\n",
    "            label = ['O' for _ in range(word_length)]\n",
    "        seq_label.extend(label)\n",
    "\n",
    "    assert len(seq_label) == len(tokens)\n",
    "    return seq_label \n",
    "\n",
    "def transform_annotations(input_annotations):\n",
    "    output_data = []\n",
    "    for idx, annotation in enumerate(input_annotations):\n",
    "        text = annotation['text']\n",
    "        labels = annotation['labels']\n",
    "        tokens = tokenize(text)\n",
    "        tags = create_tags(tokens, labels)\n",
    "\n",
    "        labels_list = [{'span': label['text'], 'type': label['label']} for label in labels]\n",
    "        transformed_annotation = {\n",
    "            'tokens': tokens,\n",
    "            'tags': tags,\n",
    "            'text': text,\n",
    "            'labels': labels_list,\n",
    "            'id': str(idx)\n",
    "        }\n",
    "        output_data.append(transformed_annotation)\n",
    "    return output_data\n",
    "\n",
    "def process_file(input_f, output_f):\n",
    "    with open(input_f, 'r', encoding='utf-8') as infile:\n",
    "        input_data = json.load(infile)\n",
    "\n",
    "    transformed_data = transform_annotations(input_data)\n",
    "\n",
    "    with open(output_f, 'w', encoding='utf-8') as outfile:\n",
    "        for item in transformed_data:\n",
    "            outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_f = 'filtered_annotations.json'\n",
    "output_f = 'demo_and_test.jsonl'\n",
    "process_file(input_f, output_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo data saved to: data/animals_or_not/demo.jsonl\n",
      "Test data saved to: data/animals_or_not/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Paths to the input and output files\n",
    "input_f = '/home/arjan_v_d/LLMarjan/demo_and_test.jsonl'\n",
    "demo_f = 'data/animals_or_not/demo.jsonl'\n",
    "test_f = 'data/animals_or_not/test.jsonl'\n",
    "\n",
    "# Function to load the input data\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Function to save the data into a jsonl file\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Load the input data\n",
    "data = load_jsonl(input_f)\n",
    "\n",
    "# Randomly shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the data\n",
    "demo_data = data[0:100]\n",
    "test_data = data[100:]\n",
    "\n",
    "# Save the split data\n",
    "save_jsonl(demo_data, demo_f)\n",
    "save_jsonl(test_data, test_f)\n",
    "\n",
    "print(f'Demo data saved to: {demo_f}')\n",
    "print(f'Test data saved to: {test_f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home/arjan_v_d/LLMarjan/data/animals_or_not/demo.jsonl\"\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Open the .jsonl file and read line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as JSON and append to the data list\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(tokens, labels):\n",
    "    tags = ['O'] * len(tokens)\n",
    "    for label in labels:\n",
    "        label_text = label['text']\n",
    "        start = label['start']\n",
    "        end = label['end']\n",
    "        entity_tokens = tokenize(label_text)\n",
    "        entity_type = label['label']\n",
    "        entity_length = len(entity_tokens)\n",
    "\n",
    "        # Find the start token index\n",
    "        char_index = 0\n",
    "        start_token_index = -1\n",
    "        for i, token in enumerate(tokens):\n",
    "            if char_index == start:\n",
    "                start_token_index = i\n",
    "                break\n",
    "            char_index += len(token) + 1 # +1 for the space or punctuation\n",
    "\n",
    "        # Assign tags\n",
    "        if start_token_index != -1:\n",
    "            for i in range(entity_length):\n",
    "                tags[start_token_index + i] = entity_type\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home/arjan_v_d/LLMarjan/data/animals_or_not/demo.jsonl\"\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Open the .jsonl file and read line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as JSON and append to the data list\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inter annotator agreement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the two annotation files\n",
    "with open('inter_annotator_agreement_annotaties_dieuwertje.json') as file:\n",
    "    annotations_dieuwertje = json.load(file)\n",
    "\n",
    "with open('inter_annotator_agreement_annotaties_thirza.json') as file:\n",
    "    annotations_thirza = json.load(file)\n",
    "\n",
    "# Create a dictionary to merge annotations by text\n",
    "merged_annotations = {}\n",
    "\n",
    "# Function to merge annotations\n",
    "def merge_annotations(text, labels1, labels2):\n",
    "    merged_labels = labels1 + labels2\n",
    "    return {\"text\": text, \"labels\": merged_labels}\n",
    "\n",
    "# Merge annotations by text\n",
    "for ann in annotations_dieuwertje:\n",
    "    text = ann[\"text\"]\n",
    "    labels = ann[\"labels\"]\n",
    "    if text not in merged_annotations:\n",
    "        merged_annotations[text] = {\"text\": text, \"labels_dieuwertje\": labels, \"labels_thirza\": []}\n",
    "    else:\n",
    "        merged_annotations[text][\"labels_dieuwertje\"] = labels\n",
    "\n",
    "for ann in annotations_thirza:\n",
    "    text = ann[\"text\"]\n",
    "    labels = ann[\"labels\"]\n",
    "    if text not in merged_annotations:\n",
    "        merged_annotations[text] = {\"text\": text, \"labels_dieuwertje\": [], \"labels_thirza\": labels}\n",
    "    else:\n",
    "        merged_annotations[text][\"labels_thirza\"] = labels\n",
    "\n",
    "# Convert merged annotations to a list\n",
    "merged_annotations_list = [\n",
    "    {\n",
    "        \"text\": text,\n",
    "        \"labels_dieuwertje\": details[\"labels_dieuwertje\"],\n",
    "        \"labels_thirza\": details[\"labels_thirza\"]\n",
    "    }\n",
    "    for text, details in merged_annotations.items()\n",
    "]\n",
    "\n",
    "# Save the merged annotations to a new JSON file\n",
    "output_file = 'agreement_annotations.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(merged_annotations_list, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Basic tokenizer that splits on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def create_tags(tokens, span_label):\n",
    "    \"\"\"\n",
    "    Convert span labels to sequence labels without BIO scheme.\n",
    "    \"\"\"\n",
    "    tags = ['O'] * len(tokens)  # Initialize all tags as 'O'\n",
    "    \n",
    "    current_pos = 0\n",
    "    token_indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_indices.append((current_pos, current_pos + len(token)))\n",
    "        current_pos += len(token) + 1  # +1 for the space or punctuation\n",
    "    \n",
    "    for entity in span_label:\n",
    "        entity_label = entity[\"label\"]\n",
    "        start_pos = entity[\"start\"]\n",
    "        end_pos = entity[\"end\"]\n",
    "        \n",
    "        for i, (start_idx, end_idx) in enumerate(token_indices):\n",
    "            if start_pos < end_idx and end_pos > start_idx:\n",
    "                tags[i] = entity_label\n",
    "\n",
    "    return tags\n",
    "\n",
    "def process_annotations(data):\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        text = entry[\"text\"]\n",
    "        tokens = tokenize(text)\n",
    "        labels_dieuwertje = entry[\"labels_dieuwertje\"]\n",
    "        labels_thirza = entry[\"labels_thirza\"]\n",
    "        \n",
    "        tags_dieuwertje = create_tags(tokens, labels_dieuwertje)\n",
    "        tags_thirza = create_tags(tokens, labels_thirza)\n",
    "        \n",
    "        overview_dieuwertje = list(zip(tokens, tags_dieuwertje))\n",
    "        overview_thirza = list(zip(tokens, tags_thirza))\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"tokens\": tokens,\n",
    "            \"tags_dieuwertje\": tags_dieuwertje,\n",
    "            \"tags_thirza\": tags_thirza,\n",
    "            \"overview_dieuwertje\": overview_dieuwertje,\n",
    "            \"overview_thirza\": overview_thirza\n",
    "        })\n",
    "    return processed_data\n",
    "\n",
    "    # Read the data from ann.json\n",
    "with open('agreement_annotations.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the annotations\n",
    "processed_data = process_annotations(data)\n",
    "  \n",
    "# Save the processed data to a new file\n",
    "with open('processed_annotations.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(processed_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert the vocabulary dictionary to a sorted list by token ID\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda item: item[1])\n",
    "\n",
    "# Open a text file for writin\n",
    "with open(\"sorted_vocabulary.txt\", \"w\") as f:\n",
    "    for token, id in sorted_vocab:\n",
    "        # Write each token and its ID to the file, sorted by ID\n",
    "        f.write(f\"{id} {token}\\n\")  # Change the order to ID first, then token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
