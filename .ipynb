{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassis\n",
    "import json\n",
    "\n",
    "\n",
    "xml_file = \"/home/arjan_v_d/LLMarjan/voorreden_annotaties/TypeSystem.xml\"\n",
    "xmi_file = \"/home/arjan_v_d/LLMarjan/voorreden_annotaties/Dieuwertje.xmi\"\n",
    "output_file = \"voorreden_annotaties.json\"\n",
    "# Load the TypeSystem\n",
    "with open(xml_file, 'rb') as f:\n",
    "    typesystem = cassis.load_typesystem(f)\n",
    "\n",
    "# Load the XMI file\n",
    "with open(xmi_file, 'rb') as f:\n",
    "    cas = cassis.load_cas_from_xmi(f, typesystem=typesystem)\n",
    "\n",
    "# Prepare the output list\n",
    "sentences_list = []\n",
    "\n",
    "# Define types\n",
    "SentenceType = typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence')\n",
    "NamedEntityType = typesystem.get_type('custom.Span')\n",
    "\n",
    "# Extract sentences and their annotations\n",
    "for sentence in cas.select(SentenceType):\n",
    "    sentence_text = cas.sofa_string[sentence.begin:sentence.end]  # Corrected attribute name here\n",
    "    labels = []\n",
    "    \n",
    "    for named_entity in cas.select_covered(NamedEntityType, sentence):\n",
    "        label_text = cas.sofa_string[named_entity.begin:named_entity.end]  # Corrected attribute name here\n",
    "        labels.append({\n",
    "            \"text\": label_text,\n",
    "            \"start\": named_entity.begin - sentence.begin,\n",
    "            \"end\": named_entity.end - sentence.begin,\n",
    "            \"label\": getattr(named_entity, 'label', 'Unknown')  # Handling missing label attribute\n",
    "        })\n",
    "    \n",
    "    sentences_list.append({\n",
    "        \"text\": sentence_text,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(sentences_list, f, indent=4)\n",
    "\n",
    "# Print the results in JSON format (for verification)\n",
    "# print(json.dumps(sentences_list, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Basic tokenizer that splits on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def create_tags(tokens, labels):\n",
    "    tags = ['O'] * len(tokens)\n",
    "    for label in labels:\n",
    "        label_text = label['text']\n",
    "        start = label['start']\n",
    "        end = label['end']\n",
    "        entity_tokens = tokenize(label_text)\n",
    "        entity_type = label['label']\n",
    "        entity_length = len(entity_tokens)\n",
    "\n",
    "        # Find the start token index\n",
    "        char_index = 0\n",
    "        start_token_index = -1\n",
    "        for i, token in enumerate(tokens):\n",
    "            if char_index == start:\n",
    "                start_token_index = i\n",
    "                break\n",
    "            char_index += len(token) + 1 # +1 for the space or punctuation\n",
    "\n",
    "        # Assign tags\n",
    "        if start_token_index != -1:\n",
    "            for i in range(entity_length):\n",
    "                tags[start_token_index + i] = entity_type\n",
    "    return tags\n",
    "\n",
    "def transform_annotations(input_annotations):\n",
    "    output_data = []\n",
    "    for idx, annotation in enumerate(input_annotations):\n",
    "        text = annotation['text']\n",
    "        labels = annotation['labels']\n",
    "        tokens = tokenize(text)\n",
    "        tags = create_tags(tokens, labels)\n",
    "        labels_list = [{'span': label['text'], 'type': label['label']} for label in labels]\n",
    "        transformed_annotation = {\n",
    "            'tokens': tokens,\n",
    "            'tags': tags,\n",
    "            'text': text,\n",
    "            'labels': labels_list,\n",
    "            'id': str(idx)\n",
    "        }\n",
    "        output_data.append(transformed_annotation)\n",
    "    return output_data\n",
    "\n",
    "def process_file(input_f, output_f):\n",
    "    with open(input_f, 'r', encoding='utf-8') as infile:\n",
    "        input_data = json.load(infile)\n",
    "\n",
    "    transformed_data = transform_annotations(input_data)\n",
    "\n",
    "    with open(output_f, 'w', encoding='utf-8') as outfile:\n",
    "        for item in transformed_data:\n",
    "            outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_f = 'voorreden_annotaties.json'\n",
    "output_f = 'demo_voorreden.jsonl'\n",
    "process_file(input_f, output_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo data saved to: /home/arjan_v_d/LLMarjan/data/planimals/demo.jsonl\n",
      "Test data saved to: /home/arjan_v_d/LLMarjan/data/planimals/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Paths to the input and output files\n",
    "input_f = '/home/arjan_v_d/LLMarjan/data/planimals/demo.jsonl'\n",
    "demo_f = '/home/arjan_v_d/LLMarjan/data/planimals/demo.jsonl'\n",
    "test_f = '/home/arjan_v_d/LLMarjan/data/planimals/test.jsonl'\n",
    "\n",
    "# Function to load the input data\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Function to save the data into a jsonl file\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Load the input data\n",
    "data = load_jsonl(input_f)\n",
    "\n",
    "# Randomly shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the data\n",
    "demo_data = data[:44]\n",
    "test_data = data[44:94]\n",
    "\n",
    "# Save the split data\n",
    "save_jsonl(demo_data, demo_f)\n",
    "save_jsonl(test_data, test_f)\n",
    "\n",
    "print(f'Demo data saved to: {demo_f}')\n",
    "print(f'Test data saved to: {test_f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags, ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
