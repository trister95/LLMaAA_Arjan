{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parse annotations from inception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotations from documents have been saved to annotations_D.json\n"
     ]
    }
   ],
   "source": [
    "import cassis\n",
    "import json\n",
    "import os\n",
    "\n",
    "base_folder = \"annotations/interrater_agreeement_D/dbnl\"\n",
    "output_file = \"annotations_D.json\"\n",
    "\n",
    "# List to hold all sentences from all documents\n",
    "all_documents_sentences = []\n",
    "\n",
    "# Iterate over each subdirectory in the base folder\n",
    "for document_name in os.listdir(base_folder):\n",
    "    document_folder = os.path.join(base_folder, document_name)\n",
    "\n",
    "    # Assuming each folder contains exactly one XML and one XMI file\n",
    "    xml_file = next(os.path.join(document_folder, f) for f in os.listdir(document_folder) if f.endswith('.xml'))\n",
    "    xmi_file = next(os.path.join(document_folder, f) for f in os.listdir(document_folder) if f.endswith('.xmi'))\n",
    "\n",
    "    # Load the TypeSystem\n",
    "    with open(xml_file, 'rb') as f:\n",
    "        typesystem = cassis.load_typesystem(f)\n",
    "\n",
    "    # Load the XMI file\n",
    "    with open(xmi_file, 'rb') as f:\n",
    "        cas = cassis.load_cas_from_xmi(f, typesystem=typesystem)\n",
    "\n",
    "    # Define types\n",
    "    SentenceType = typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence')\n",
    "    NamedEntityType = typesystem.get_type('custom.Span')\n",
    "\n",
    "    # Prepare the output list for this document\n",
    "    sentences_list = []\n",
    "\n",
    "    # Extract sentences and their annotations\n",
    "    for sentence in cas.select(SentenceType):\n",
    "        sentence_text = cas.sofa_string[sentence.begin:sentence.end]\n",
    "        labels = []\n",
    "        \n",
    "        for named_entity in cas.select_covered(NamedEntityType, sentence):\n",
    "            label_text = cas.sofa_string[named_entity.begin:named_entity.end]\n",
    "            labels.append({\n",
    "                \"text\": label_text,\n",
    "                \"start\": named_entity.begin - sentence.begin, \n",
    "                \"end\": named_entity.end - sentence.begin,\n",
    "                \"label\": getattr(named_entity, 'label', 'Unknown')\n",
    "            })\n",
    "        \n",
    "        sentences_list.append({\n",
    "            \"text\": sentence_text,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "    # Append results from this document to the all documents list\n",
    "    all_documents_sentences.extend(sentences_list)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_documents_sentences, f, indent=4)\n",
    "\n",
    "# Optionally print the results\n",
    "print(f\"All annotations from documents have been saved to {output_file}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optional: filter labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON data has been saved to filtered_documents_annotaties_no_labels.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the existing JSON data\n",
    "input_file = 'all_documents_annotaties.json'\n",
    "output_file = 'filtered_documents_annotaties_no_labels.json'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter labels for each document to only include those with 'An-Org-Lit'\n",
    "filtered_data = []\n",
    "for document in data:\n",
    "    # Filter labels to keep only 'An-Org-Lit'\n",
    "    filtered_labels = [label for label in document['labels'] if label['label'] == 'An-Org-Lit']\n",
    "    # Append the document with only the filtered labels\n",
    "    filtered_data.append({\n",
    "        \"text\": document['text'],\n",
    "        \"labels\": filtered_labels\n",
    "    })\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "print(f\"Filtered JSON data has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convert to NER format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Basic tokenizer that splits on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def create_tags(tokens, span_label):\n",
    "    \"\"\"\n",
    "    Covert span labels to sequence labels.\n",
    "    Language: en/zh\n",
    "    \"\"\"\n",
    "    if span_label != []:\n",
    "        for e in span_label:\n",
    "            e[\"span\"] = e[\"text\"]\n",
    "            e[\"type\"] = e[\"label\"]\n",
    "    span_label = sorted(span_label, key=lambda x: len(x['span']), reverse=True)\n",
    "    span_to_type = {entity['span']: entity['type'] for entity in span_label}\n",
    "    # get words list\n",
    "\n",
    "    # build a tokenizer first\n",
    "    dictionary = dict()\n",
    "    for token in tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary[token] = f'[{len(dictionary)}]'\n",
    "    id_string = ' '.join([dictionary[token] for token in tokens])\n",
    "    for entity in span_label:\n",
    "        span_tokens = entity['span'].strip().split(' ')\n",
    "        # validate span token\n",
    "        valid_flag = True\n",
    "        for token in span_tokens:\n",
    "            if token not in dictionary:\n",
    "                valid_flag = False\n",
    "                break\n",
    "        if not valid_flag:\n",
    "            continue\n",
    "        # translate span token into ids\n",
    "        id_substring = ' '.join([dictionary[token] for token in span_tokens])\n",
    "        id_string = ('[sep]' + id_substring + '[sep]').join(id_string.split(id_substring))\n",
    "        # print(id_string)\n",
    "    # convert back to nl\n",
    "    sent = id_string\n",
    "    for token in dictionary:\n",
    "        sent = sent.replace(dictionary[token], token)\n",
    "    words = sent.split('[sep]')\n",
    "\n",
    "    seq_label = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        entity_flag = (word in span_to_type)\n",
    "        word_length = len(word.split(' '))\n",
    "        if entity_flag:\n",
    "            if word_length == 1:\n",
    "                label = [f'{span_to_type[word]}']\n",
    "            else:\n",
    "                label = ([f'{span_to_type[word]}'] * (word_length))\n",
    "        else:\n",
    "            label = ['O' for _ in range(word_length)]\n",
    "        seq_label.extend(label)\n",
    "\n",
    "    assert len(seq_label) == len(tokens)\n",
    "    return seq_label \n",
    "\n",
    "def transform_annotations(input_annotations):\n",
    "    label_mapping = {\n",
    "    \"An-Org-Lit\": \"Animals-Organisms-Literal\",\n",
    "    \"An-Org-Sym\": \"Animals-Organisms-Symbolical\",\n",
    "    \"An-Org-Petrified\": \"Animals-Organisms-Petrified\",\n",
    "    \"An-Part-Lit\": \"Animals-Parts-Literal\",\n",
    "    \"An-Part-Sym\": \"Animals-Parts-Symbolical\",\n",
    "    \"An-Part-Petrified\": \"Animals-Parts-Petrified\",\n",
    "    \"An-Prod-Lit\": \"Animals-Products-Literal\",\n",
    "    \"An-Prod-Sym\": \"Animals-Products-Symbolical\",\n",
    "    \"An-Prod-Petrified\": \"Animals-Products-Petrified\",\n",
    "    \"An-Coll-Lit\": \"Animals-Collective-Literal\",\n",
    "    \"An-Coll-Sym\": \"Animals-Collective-Symbolical\",\n",
    "    \"An-Coll-Petrified\": \"Animals-Collective-Petrified\",\n",
    "    \"Plant-Org-Lit\": \"Plants-Organisms-Literal\",\n",
    "    \"Plant-Org-Sym\": \"Plants-Organisms-Symbolical\",\n",
    "    \"Plant-Org-Petrified\": \"Plants-Organisms-Petrified\",\n",
    "    \"Plant-Part-Lit\": \"Plants-Parts-Literal\",\n",
    "    \"Plant-Part-Sym\": \"Plants-Parts-Symbolical\",\n",
    "    \"Plant-Part-Petrified\": \"Plants-Parts-Petrified\",\n",
    "    \"Plant-Prod-Lit\": \"Plants-Products-Literal\",\n",
    "    \"Plant-Prod-Sym\": \"Plants-Products-Symbolical\",\n",
    "    \"Plant-Prod-Petrified\": \"Plants-Products-Petrified\",\n",
    "    \"Plant-Coll-Literal\": \"Plants-Collective-Literal\",\n",
    "    \"Plant-Coll-Sym\": \"Plants-Collective-Symbolical\",\n",
    "    \"Plant-Coll-Petrified\": \"Plants-Collective-Petrified\"\n",
    "    }\n",
    "    output_data = []\n",
    "    for idx, annotation in enumerate(input_annotations):\n",
    "        text = annotation['text']\n",
    "        labels = annotation['labels']\n",
    "        \n",
    "        # Update the labels according to the mapping\n",
    "        updated_labels = []\n",
    "        for label in labels:\n",
    "            label_type = label['label']\n",
    "            if label_type in label_mapping:\n",
    "                label_type = label_mapping[label_type]\n",
    "            updated_labels.append({'text': label['text'], 'label': label_type})\n",
    "\n",
    "        tokens = tokenize(text)\n",
    "        tags = create_tags(tokens, updated_labels)\n",
    "\n",
    "        labels_list = [{'span': label['text'], 'type': label['label']} for label in updated_labels]\n",
    "\n",
    "        transformed_annotation = {\n",
    "            'tokens': tokens,\n",
    "            'tags': tags,\n",
    "            'text': text,\n",
    "            'labels': labels_list,\n",
    "            'id': str(idx)\n",
    "        }\n",
    "        output_data.append(transformed_annotation)\n",
    "    return output_data\n",
    "\n",
    "def process_file(input_f, output_f):\n",
    "    with open(input_f, 'r', encoding='utf-8') as infile:\n",
    "        input_data = json.load(infile)\n",
    "\n",
    "    transformed_data = transform_annotations(input_data)\n",
    "\n",
    "    with open(output_f, 'w', encoding='utf-8') as outfile:\n",
    "        for item in transformed_data:\n",
    "            outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_f = 'annotations_T.json'\n",
    "output_f = 'demo_and_test_T.jsonl'\n",
    "process_file(input_f, output_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo data saved to: data/by_the_horns_T/demo.jsonl\n",
      "Test data saved to: data/by_the_horns_T/test.jsonl\n",
      "Holdout data saved to: data/by_the_horns_T/holdout.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Paths to the input and output files\n",
    "input_f = 'demo_and_test_T.jsonl'\n",
    "demo_f = 'data/by_the_horns_T/demo.jsonl'\n",
    "test_f = 'data/by_the_horns_T/test.jsonl'\n",
    "holdout_f = 'data/by_the_horns_T/holdout.jsonl'\n",
    "\n",
    "# Function to load the input data\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# Function to save the data into a jsonl file\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Load the input data\n",
    "data = load_jsonl(input_f)\n",
    "\n",
    "# Randomly shuffle the data\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split the data\n",
    "demo_data = data[0:100]\n",
    "test_data = data[100:150]\n",
    "holdout_data = data[150:]\n",
    "\n",
    "# Save the split data\n",
    "save_jsonl(demo_data, demo_f)\n",
    "save_jsonl(test_data, test_f)\n",
    "save_jsonl(holdout_data, holdout_f)\n",
    "\n",
    "print(f'Demo data saved to: {demo_f}')\n",
    "print(f'Test data saved to: {test_f}')\n",
    "print(f'Holdout data saved to: {holdout_f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
