{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def fix_short_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    adjusted_sentences = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        words = sentences[i].split()\n",
    "        # Merge sentences with fewer than 10 words with the next one\n",
    "        while len(words) < 10 and i + 1 < len(sentences):\n",
    "            i += 1\n",
    "            words += sentences[i].split()\n",
    "        i += 1\n",
    "        adjusted_sentences.append(' '.join(words))\n",
    "    return adjusted_sentences\n",
    "\n",
    "def split_list_equally(lst, n_parts):\n",
    "    # Calculate size of smallest and largest parts\n",
    "    part_size = len(lst) // n_parts\n",
    "    remainder = len(lst) % n_parts\n",
    "    \n",
    "    # Generate the split indices\n",
    "    split_indices = [part_size] * n_parts\n",
    "    for i in range(remainder):\n",
    "        split_indices[i] += 1\n",
    "    \n",
    "    # Use the indices to split the list\n",
    "    current_index = 0\n",
    "    result = []\n",
    "    for size in split_indices:\n",
    "        result.append(lst[current_index:current_index + size])\n",
    "        current_index += size\n",
    "    return [\" \".join(e)for e in result]\n",
    "\n",
    "\n",
    "def fix_long_sentences(sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        words = s.split()\n",
    "        length = len(words)\n",
    "        if length<=100:\n",
    "            new_sentences.append(s)\n",
    "        if length > 100:\n",
    "            no_of_splits = length //100 + 1\n",
    "            split_sentences = split_list_equally(words, no_of_splits)\n",
    "            for spl_sent in split_sentences:\n",
    "                new_sentences.append(spl_sent)\n",
    "    return new_sentences        \n",
    "\n",
    "file_path = \"geestelijck_bloemhofken.txt\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "while \"  \" in text:\n",
    "    text = text.replace(\"  \",\" \")\n",
    "\n",
    "fixed_for_short_sentences = fix_short_sentences(text)\n",
    "fixed_for_long_sentences = fix_long_sentences(fixed_for_short_sentences)\n",
    "\n",
    "# Save the cleaned final sentences to a new file\n",
    "output_file_path_cleaned = 'geestelijk_bloemhofken_chunks.txt'\n",
    "with open(output_file_path_cleaned, 'w', encoding='utf-8') as file:\n",
    "    for sentence in fixed_for_long_sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
